{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq easy-vqa\n",
    "!pip install -qqq sentence_transformers transformers timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_vqa import get_train_questions, get_test_questions\n",
    "\n",
    "train_questions,train_answers,train_image_ids=get_train_questions()\n",
    "test_questions,test_answers,test_image_ids=get_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "def gen_dataframes(questions,answers,image_ids,mode=\"train\"):\n",
    "    records=[]\n",
    "    for question,answer, image_id in zip(questions,answers,image_ids):\n",
    "        image_path=f\"/usr/local/lib/python3.7/dist-packages/easy_vqa/data/{mode}/images/{image_id}.png\"\n",
    "        records.append({\"question\" : question, \"answer\": answer, \"image_path\": image_path})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df=gen_dataframes(train_questions,train_answers,train_image_ids)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=df.sample(frac=1)\n",
    "train_df,eval_df = train_test_split(df)\n",
    "test_df=gen_dataframes(test_questions,test_answers,test_image_ids,mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28931, 3)\n",
      "(9644, 3)\n",
      "(9673, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(eval_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Labels:  13\n"
     ]
    }
   ],
   "source": [
    "from easy_vqa import get_answers\n",
    "\n",
    "answers=get_answers()\n",
    "print(\"Total Labels: \",len(answers))\n",
    "label2idx={answer:i for i, answer in enumerate(answers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'circle': 0,\n",
       " 'green': 1,\n",
       " 'red': 2,\n",
       " 'gray': 3,\n",
       " 'yes': 4,\n",
       " 'teal': 5,\n",
       " 'black': 6,\n",
       " 'rectangle': 7,\n",
       " 'yellow': 8,\n",
       " 'triangle': 9,\n",
       " 'brown': 10,\n",
       " 'blue': 11,\n",
       " 'no': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"]=train_df[\"answer\"].apply(lambda x: label2idx.get(x))\n",
    "eval_df[\"label\"]=eval_df[\"answer\"].apply(lambda x: label2idx.get(x))\n",
    "test_df[\"label\"]=test_df[\"answer\"].apply(lambda x: label2idx.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25849</th>\n",
       "      <td>does the image contain a triangle?</td>\n",
       "      <td>yes</td>\n",
       "      <td>/usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/2689.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6937</th>\n",
       "      <td>is a triangle present?</td>\n",
       "      <td>no</td>\n",
       "      <td>/usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/711.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18399</th>\n",
       "      <td>is there a red shape in the image?</td>\n",
       "      <td>no</td>\n",
       "      <td>/usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/1915.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33639</th>\n",
       "      <td>is there not a blue shape?</td>\n",
       "      <td>yes</td>\n",
       "      <td>/usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/3490.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38196</th>\n",
       "      <td>is a black shape present?</td>\n",
       "      <td>no</td>\n",
       "      <td>/usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/3959.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question answer  \\\n",
       "25849  does the image contain a triangle?    yes   \n",
       "6937               is a triangle present?     no   \n",
       "18399  is there a red shape in the image?     no   \n",
       "33639          is there not a blue shape?    yes   \n",
       "38196           is a black shape present?     no   \n",
       "\n",
       "                                                                       image_path  \n",
       "25849  /usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/2689.png  \n",
       "6937    /usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/711.png  \n",
       "18399  /usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/1915.png  \n",
       "33639  /usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/3490.png  \n",
       "38196  /usr/local/lib/python3.7/dist-packages/easy_vqa/data/train/images/3959.png  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sujit\\.conda\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:29<00:00, 14.9MB/s] \n",
      "c:\\Users\\Sujit\\.conda\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sujit\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 160/160 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 502/502 [00:00<?, ?B/s] \n",
      "c:\\Users\\Sujit\\.conda\\envs\\llm\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [00:23<00:00, 14.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModel\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "\"\"\"Fusing Transformers\"\"\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text_encoder=AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad=False\n",
    "\n",
    "image_processor=AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "image_encoder=AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "\"\"\"Fusing CNNs and Transformers\"\"\"\n",
    "# device= \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# image_encoder=timm.create_model(\"resnet50d\",pretrained=True, num_classes=0)\n",
    "# resize_transform=T.resize((224,224))\n",
    "\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad=False\n",
    "\n",
    "image_encoder.to(device)\n",
    "text_encoder.to(device)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 1, 2, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Stitch torch dataset with feature backbones\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class EasyQADataset(Dataset):\n",
    "\n",
    "    def __init__(self, df,\n",
    "                 image_encoder,\n",
    "                 text_encoder,\n",
    "                 image_processor,\n",
    "                 tokenizer,\n",
    "                ):\n",
    "        self.df=df\n",
    "        self.image_encoder=image_encoder\n",
    "        self.text_encoder=text_encoder\n",
    "        self.image_processor=image_processor\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _getitem__(self,idx):\n",
    "        image_file=self.df[\"image_path\"][idx]\n",
    "        question=self.df[\"question\"][idx]\n",
    "        image=Image.open(image_file).convert(\"RGB\")\n",
    "        label=self.df[\"label\"][idx]\n",
    "\n",
    "        \"\"\"When CNNs are used for V backnone\"\"\"\n",
    "\n",
    "        image=resize_transform(image)\n",
    "        image_inputs=T.ToTensor()(image).unsqueeze_(0)\n",
    "\n",
    "        \"\"\"When Transformers is used for V backbone\"\"\"\n",
    "        image_inputs=self.image_processor(image,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
